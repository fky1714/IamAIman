---
title: "Google's New System for Generating Secure, AI-Made Data: An Overview"
categories:
  - Artificial Intelligence
tags:
  - Machine Learning
  - Security
  - Synthetic Data
  - Data Privacy
  - Google DeepMind
  - Differential Privacy
---
In the wake of growing concerns over data privacy, Google has introduced an innovative system for the creation of differentially private synthetic data. This method, chiefly applied in the creation of training data for on-device safety classification, makes use of large-language models (LLMs) and differentially private stochastic gradient descent (DP-SGD)^[1^]. 

Differential privacy is a formal mathematical framework that ensures the protection of individualsÅf privacy by rendering the output of an algorithm similar whether a certain userÅfs data is taken into account or not. Thus, the details hidden within the users' data are not revealed by the predictive model or its predictions. The tools used in this system add a layer of privacy that cannot be compromised^[2^]. 

Key to this process is the privacy-preserving method of LLM 'fine-tuning'. This is achievable by modifying the input used by the LLM rather than the model parameters, known as Åeprompt fine-tuningÅf. This parameter-efficient fine-tuning technique speeds up computations while preserving the quality of synthetic data^[3^].

In application, the said system has been used by Google during the development of its mobile foundation model. The model's output is overseen by a high-precision safety classifier trained on a differentially private synthetic version of unsafe content, ensuring content suitability and compliance with user privacy.

# References
1. [Data privacy in Machine Learning](#)
2. [Introduction to Differential Privacy](#)
3. [Efficient Fine-Tuning in LLMs](#)