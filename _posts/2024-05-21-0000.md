---
title: "AIのプライバシー保護: 差分プライバシーを用いた人工的訓練データの生成"
categories:
  - Machine Learning
tags:
  - AI
  - Privacy
  - Training Data
  - Differential Privacy
---

データとプライバシーの関連性が重要になる現代、Googleではユーザーのプライバシーを保護しながら効果的な機械学習モデルを訓練する方法として、差分プライバシーを用いた人工的（シンセティック）訓練データの生成に取り組んでいます[^1^]。

機械学習 (Machine Learning) モデルは、大量のデータセットを用いて訓練されることで優れた予測性能を発揮します[^2^]。しかし、これらのデータセットには個々のユーザーの詳細な情報が含まれているため、モデルの訓練や予測によって個人のプライバシーが漏洩する可能性があります。ここで使用されるテクニックが、差分プライバシー (Differential Privacy) です[^3^]。

差分プライバシーは、データセットからプライバシーを保護しながら有用な情報を得るためのフレームワークです。差分プラバシーを適用したアルゴリズムを使用すると、ユーザーのデータがデータセットに含まれているか否かに関わらず、結果はほとんど変わらないため、ユーザーの情報が漏れることはありません。

差分プライバシーを機械学習に組み込む一つの手法が、差分プライバシー確率的勾配降下法 (DP-SGD) です[^4^]。このアルゴリズムを用いてモデルを訓練すると、出力モデルやその予測からユーザーのデータを特定することはできません。

さらに、Googleでは大規模言語モデル (large-language models, LLMs) を用いて、差分プライバシーに基づく人工的なデータ、シンセティックデータを生成する技術を開発しており[^5^]、この技術は既存のデータセットに含まれる特性を保持したまま、人工的なデータを生成します。このため、シンセティックデータは元のデータセットと同じような行動を示すモデルの訓練に使用でき、モデルの品質を損なうことなくプライバシーを保護できます。

Googleの取り組み includeは、パラメータ効率的な微調整 (parameter-efficient fine-tuning) テクニックを用いたLLMsによるシンセティックデータの生成です。このテクニックでは、LLMsのパラメータの一部を微調整し、LoRa微調整 (LoRa fine-tuning) やプロンプト微調整 (prompt fine-tuning) といった手法を使用しています[^6^][^7^]。

これらの方法を用いて、Googleはシンセティックデータを用いた効果的な機械学習モデルの訓練を実現しています。具体的には、Lambda-8Bという大規模なモデルを利用し、パラメータ効率的な微調整を用いた方法でモデルを訓練しています[^8^]。そして、生成されたシンセティックデータを利用して、予測分類器 (predictive classifiers) を訓練し、元のデータに基づく分類器と比較しています[^9^]。

さらに、このテクニックは、新たに発表されたGoogleのモバイルデバイス向け基盤モデルのセーフティ分類にも適用されています。これにより、モデルが生成するすべてのレスポンスがユーザーに適していることを高精度のトランスフォーマーベースのセーフティ分類器で確認しています[^10^][^11^]。

これまでに示したように、差分プライバシーを用いたシンセティックデータの生成は、ユーザーのプライバシーを保護しながら、人工的なデータを使用して機械学習モデルを訓練するための有効な手法となりました。Googleはこのテクニックにより、ユーザーのプライバシーを尊重しつつ、高品質な機械学習モデルの訓練を可能にしています。このような技術は、私たちがデータを活用しながらも、プライバシーへの配慮を忘れない重要性を示しています。

参照URL:
1. ブログ記事: Protecting users with differentially private synthetic training data[^1^]
2. 機械学習とは[^2^]
3. 差分プライバシーとは[^3^]
4. 差分プライバシー確率的勾配降下法 (DP-SGD) とは[^4^]
5. 大規模言語モデル (large-language models, LLMs) とは[^5^]
6. LoRa微調整 (LoRa fine-tuning) とは[^6^]
7. プロンプト微調整 (prompt fine-tuning) とは[^7^]
8. Lamda-8Bとは[^8^]
9. 予測分類器 (predictive classifiers) とは[^9^]
10. オンデバイスセーフティ分類とは[^10^]
11. トランスフォーマーベースのセーフティ分類器とは[^11^]
[^1^]:https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/
[^2^]:https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92
[^3^]:https://www.anonify.layerx.co.jp/post/differential-privacy
[^4^]:https://qiita.com/toshi_4886/items/ba8d63d780951624ca42
[^5^]:https://atmarkit.itmedia.co.jp/ait/articles/2303/13/news013.html
[^6^]:https://www.brainpad.co.jp/doors/contents/01_tech_2023-05-22-153000/
[^7^]:https://qiita.com/tmgauss/items/22c4e5e00282a23e569d
[^8^]:https://aws.amazon.com/jp/lambda/pricing/
[^9^]:https://brb.nci.nih.gov/techreport/MAQC-2.pdf
[^10^]:https://www.fda.gov/medical-devices/overview-device-regulation/classify-your-medical-device
[^11^]:https://arxiv.org/abs/2403.18223
