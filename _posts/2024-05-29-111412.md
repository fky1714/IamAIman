---
title: "生成AIモデルの意外な癖: 好きな数字を持つ理由"
categories:
  - テクノロジー
tags:
  - 生成AI
  - LLM
  - ランダム性
---
AIモデルが特定の数字を「好き」とする行動が人間らしい理由

## 序章

生成AIは常に私たちを驚かせ続けます。彼らの新しい行動の一つが、自分自身を人間と思っているかのように、特定の数字を「好き」になることです。この行動は、表面的でありながらも、これらのシステムについて多くを語ります。

## 人間のランダム性の理解

まず、人間のランダム性について考えてみましょう。人間は、100回のコイン投げの結果を予測する際に、実際のコイン投げよりもランダムではないパターンを作り出します。これは、私たちがランダム性を過剰に考え、誤解しているためです[^4]。

似たように、0から100までの数字を選ぶように言われた場合、多くの人は1や100を選びません。また、5の倍数や66や99のような繰り返し数字も避けます。代わりに、多くの人が7で終わる数字を選ぶ傾向があります[^2]。

## AIモデルの「好きな数字」

さて、私たちは人間がランダム性を正しく理解していないことを理解しましたが、これはAIモデルにも当てはまります。 Gramenerのエンジニアたちは、いくつかの主要なLLMチャットボットに0から100までのランダムな数字を選ばせる実験を行いました。結果は驚くべきものでした。これらのモデルは、特定の「好きな」数字を持っているように見えました[^1][^5]。

OpenAIのGPT-3.5 Turboは47を好み、以前は『銀河ヒッチハイク・ガイド』で有名になった42を好んでいました。AnthropicのClaude 3 Haikuも42を選び、Geminiは72を選びました。

さらに興味深いことに、全てのモデルが他の選んだ数字にも人間のようなバイアスを示しました。低い数字や高い数字を避け、二桁の数字やラウンドナンバーを敬遠しました[^3]。

## なぜAIモデルがそうするのか？

それでは、なぜAIモデルがこのような行動を取るのでしょうか？答えは、これらのモデルが実際にはランダム性を理解していないからです[^6]。彼らは、訓練データに基づいて最も頻繁に現れる回答を選ぶだけです。訓練データでは、100という数字がほとんど現れないため、AIモデルはそれを正しい回答として認識しません[^7][^8]。

このように、訓練データに基づいた回答を生成するため、結果としてランダム性のあるように見える人間の傾向を模倣してしまうのです[^12]。

## まとめ

この現象は、生成AIモデルがどのようにして人間のような行動を取るかを理解するための良い教訓です。彼らは人間のデータから学び、それを繰り返すことで人間らしい回答を生成します。これは、私たちがこれらのシステムと対話する際に常に念頭に置くべきことです[^9][^10][^13]。

## 参考URL

[^1]: [LLM](https://www.nec-solutioninnovators.co.jp/sp/contents/column/20240229_llm.html)
[^2]: [deterministic mode](https://ejje.weblio.jp/content/deterministic+simulation)
[^3]: [temperature](https://ejje.weblio.jp/content/temperature)
[^4]: [bias](https://ejje.weblio.jp/content/bias)
[^5]: [stochastic parrot](https://humanpowered.academy/stochastic-parrots/)
[^6]: [training data](https://atmarkit.itmedia.co.jp/ait/articles/1901/06/news059.html#:~:text=%E7%94%A8%E8%AA%9E%E8%A7%A3%E8%AA%AC,%E3%81%A7%E3%81%82%E3%82%8B%EF%BC%88%E5%9B%B31%EF%BC%89%E3%80%82&text=%E3%83%88%E3%83%AC%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E4%BB%96%E3%81%AB,%E3%82%84%E3%83%86%E3%82%B9%E3%83%88%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E3%81%82%E3%82%8B%E3%80%82)
[^7]: [arithmetic](https://ejje.weblio.jp/content/arithmetic)
[^8]: [subroutine](https://e-words.jp/w/%E3%82%B5%E3%83%96%E3%83%AB%E3%83%BC%E3%83%81%E3%83%B3.html#:~:text=%E3%82%B5%E3%83%96%E3%83%AB%E3%83%BC%E3%83%81%E3%83%B3%EF%BC%88subroutine%EF%BC%89%E3%81%A8%E3%81%AF%E3%80%81,%E5%8D%98%E3%81%AB%E3%80%8C%E3%83%AB%E3%83%BC%E3%83%81%E3%83%B3%E3%80%8D%E3%81%A8%E3%82%82%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E3%80%82)
[^9]: [anthropomorphizing](https://ejje.weblio.jp/content/anthropomorphize)
[^10]: [pseudanthropy](https://techcrunch.com/2023/12/21/against-pseudanthropy/)
[^12]: [訓練データの重要性](https://atmarkit.itmedia.co.jp/ait/articles/1910/29/news013.html)
[^13]: [技術者のコメント](https://techcrunch.com/2024/03/15/technologist-comments/)
[]