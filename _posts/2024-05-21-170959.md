---
title: "ユーザーのプライバシーを守る！合成トレーニングデータの革新的手法"
categories:
  - テクノロジー
tags:
  - 合成データ
  - プライバシー保護
  - 機械学習
---
機械学習モデルのオープンソース化や製品への応用が進む中、プライバシーを保護しながらデータを活用する方法が求められています。Googleの研究チームが提案する差分プライバシーに基づいた合成データの生成方法とは？

## 差分プライバシーとは？
差分プライバシーとは、データ分析アルゴリズムが個々のデータの存在や不在に関係なく出力をほぼ同一に保つ仕組みです。これにより、特定の個人のデータがアルゴリズム結果から逆算されるリスクを防ぎます[^1]。

## 合成データの重要性
合成データとは、オリジナルのデータの特徴を保持しつつ、実際のデータとは異なる人工的なデータを生成する技術です。そのため、デリケートな情報を含むデータでも、外部に提供しやすくなります[^2]。例えば、モデルのトレーニング、特徴量エンジニアリング、ハイパーパラメータチューニング[^3]などに使用できます。

## データ生成の手法
Googleでは、DP-SGD（Differentially Private Stochastic Gradient Descent）[^4]を使用してLLM（大規模言語モデル）[^5]をファインチューニングし、合成データを生成しています。これにより、大量のトレーニングデータを使わなくても、効率的な合成データが作成できます。

## パラメータ効率的ファインチューニング
従来のファインチューニング方法では、モデルの全ての重みを変更していましたが、これは計算コストが高く、結果が不十分なことが多いです。そこで、「LoRa（Low-Rank Adaptation）」[^6]と「プロンプトファインチューニング」[^7]などのパラメータ効率的手法が採用されました。

- **LoRa ファインチューニング**：モデルの重み行列WをW + LRとして表現し、低ランク行列LとRのみをトレーニングする方法です。
- **プロンプトファインチューニング**：ネットワークのスタート地点に「プロンプトテンソル」を挿入し、その重みのみを変更する手法です。

## 実証結果と最適化
これらの手法により、従来の方法よりも高品質な合成データが生成されました。特にLoRa ファインチューニングにおいて、必要なノイズの量を減らすことでDP-SGDのパフォーマンス向上が確認されました。

具体的には、IMDB映画レビュー、Yelpビジネスレビュー、AGニュース記事などのデータセットを用いて実験が行われ、それらのデータセットから生成された合成データを使って予測モデルを構築しました[^8]。その結果、合成データでトレーニングされたモデルが、元のデータでトレーニングされたモデルよりも高い精度を示すことがありました[^9]。

## 実用例：オンデバイスの安全性分類
Googleの最新のモバイルデバイス向けモデルでは、トランスフォーマーモデル[^10]を用いた高精度の安全性分類器が使用されています。敏感な内容を含むデータの安全性を保つため、差分プライバシーによる合成データを利用して分類器をトレーニングしました[^11]。これにより、データの操作やフィルタリングが容易になり、性能が向上しています。

## まとめ
差分プライバシーに基づく合成データ生成法は、プライバシー保護とデータの有効活用の両方を実現する革新的なアプローチです。これにより、機械学習のトレーニングデータの安全性が大幅に向上します。

## 参照URL
[^1]: [Differential privacy](https://www.anonify.layerx.co.jp/post/differential-privacy)
[^2]: [Synthetic data](https://acompany.tech/privacytechlab/synthetic-data-merit-demerit/#:~:text=%E5%90%88%E6%88%90%E3%83%87%E3%83%BC%E3%82%BF%E3%81%A8%E3%81%AF%E3%80%81%E3%80%8C%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E3%83%BC,AI%E8%A3%BD%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E3%81%82%E3%82%8B%E3%80%82)
[^3]: [Hyperparameter tuning](https://aws.amazon.com/jp/what-is/hyperparameter-tuning/#:~:text=%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E3%83%88%E3%83%AC%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0,%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82)
[^4]: [Stochastic gradient descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#:~:text=Stochastic%20gradient%20descent%20(often%20abbreviated,(e.g.%20differentiable%20or%20subdifferentiable).)
[^5]: [Large-language models (LLMs)](https://atmarkit.itmedia.co.jp/ait/articles/2303/13/news013.html)
[^6]: [Parameter-efficient fine-tuning](https://www.brainpad.co.jp/doors/contents/01_tech_2023-05-22-153000/)
[^7]: [Word-level convolutional neural network](https://medium.com/@joyceye04/text-understanding-with-convolutional-neural-network-with-hands-on-coding-7c03a0782a2f)
[^8]: [BERT embeddings](https://qiita.com/anyai_corp/items/1d66feea6102c28dd077)
[^9]: [Synthetic data](https://acompany.tech/privacytechlab/synthetic-data-merit-demerit/#:~:text=%E5%90%88%E6%88%90%E3%83%87%E3%83%BC%E3%82%BF%E3%81%A8%E3%81%AF%E3%80%81%E3%80%8C%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E3%83%BC,AI%E8%A3%BD%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E3%81%82%E3%82%8B%E3%80%82)
[^10]: [Transformer-based models](https://blogs.nvidia.co.jp/2022/04/13/what-is-a-transformer-model/)
[^11]: [Synthetic data](https://acompany.tech/privacytechlab/synthetic-data-merit-demerit/#:~:text=%E5%90%88%E6%88%90%E3%83%87%E3%83%BC%E3%82%BF%E3%81%A8%E3%81%AF%E3%80%81%E3%80%8C%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E3%83%BC,AI%E8%A3%BD%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E3%81%82%E3%82%8B%E3%80%82)