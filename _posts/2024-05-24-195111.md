---
title: "LANISTR：構造化データと非構造化データを活用した新しいマルチモーダル学習フレームワーク"
categories:
  - 技術
tags:
  - 生成AI
  - 機械学習
---
Googleの研究チームが発表したLANISTRは、構造化データと非構造化データを混合して学習するための新しいフレームワークです。これにより、データの多様性に対応し、精度の高い予測を実現します。

## LANISTRの概要

LANISTRは、言語（テキスト）、画像、時間系列データ、表形式データといった多様なデータ形式を取り込み、これらのデータを整合させて融合し、最終的に分類予測を生成するマルチモーダル学習のためのフレームワークです[^1][^2]。従来のマルチモーダル学習は主に非構造化データに焦点を当てていましたが、LANISTRは構造化データと非構造化データの融合を効果的に行うことで、新たな可能性を開きます[^3][^4]。

## 主要な課題と解決策

1. **データの多様性と次元数の増加**
   ディープニューラルネットワークは、入力特徴の次元数と多様化が増加すると、過学習と一般化の問題が顕著になります[^5][^6]。特に非構造化データと構造化データ（例：ファッショントレンドやセンシング測定などの時間系列データ）の組み合わせは、モデルの構築を複雑にします[^7]。LANISTRは注意機構に基づくアーキテクチャを採用することで、これらの課題に対応しています。

2. **欠落データの処理**
   マルチモーダルデータにおいて、一部のモダリティが欠落している場合、特に3つ以上のモダリティが存在するシナリオでは問題が深刻化します。LANISTRはマスクベースのトレーニング手法を用いることで、欠損データの影響を最小限に抑えます[^8]。

## LANISTRのアーキテクチャ

LANISTRは、モダリティ固有のエンコーダとマルチモーダルエンコーダデコーダモジュールから構成されています。初めに、言語、画像、構造化データ（時間系列データと表形式データ用の2つのエンコーダ）をそれぞれのエンコーダでエンコードします[^9][^10]。その後、これらのエンコードされたデータを単一層の投影ヘッドを用いてプ ロジェクトし、それらの埋め込みを連結して、マルチモーダル融合モジュールに供給します[^11]。

LANISTRの核となる部分は、ユニモーダルとマルチモーダルの両レベルで適用されるマスクベースのトレーニング戦略にあります。これにより、欠損したモダリティがあるデータでも訓練可能です[^12]。

### モダリティ固有のマスキング目標

各ユニモーダルエンコーダには、マスクされた言語、画像、時間系列、および表形式データ特徴のモデリングが使用されます[^13]。これにより、ユニモーダルエンコーダは欠損データを含む入力を基に訓練され、再構築または予測タスクが可能となります[^14]。

### 類似性ベースのマルチモーダルマスキングロス

LANISTRは新しい類似性ベースのマルチモーダル学習損失を導入しており、マスクされたデータとマスクされていないデータの埋め込みの類似性を最大化します[^15]。こ の目的は、埋め込みが類似するようにモデルを訓練し、クロスモーダル関係を学習させることにあります[^16]。

## 実際の応用例と結果

LANISTRは、医療診断や小売需要予測などの現実的なシナリオにおいて、その効果を発揮しています。2つの公開医療データセットと小売データセットを使用し、LANISTRはわずか0.1％および0.01％のラベル付きデータでファインチューニングすることで、顕著な改善を示しました[^17]。特に、35.7％および99.8％のサンプルが全てのモダリ ティを含まないきわめて高い比率でも、その堅牢性を発揮しています。

### モデルの性能比較

LANISTRは、MIMIC-IV（臨床予測タスクのために広く使用されている医療データセット）およびAmazonレビューのデータセットを使用して他の競争力のあるベースライン（AutoGluon、ALBEF、MedFuseなど）と比較され、いずれも優れた結果を示しました[^18]。例えば、MIMIC-IVデータセットを使用した院内死亡予測において、LANISTRは平均87.37％のAUROC（受信者動作特性曲線下面積）を達成し、他のベースラインモデルを大きく上回りました[^19]。

同様に、Amazonレビューのデータセットでの製品評価予測では、LANISTRは平均76.27％の精度を達成し、他のベースラインを大幅に上回りました[^20]。

## まとめ

LANISTRは、言語、画像、構造化データといった多様なデータを組み合わせることで、欠損データや限られたラベル付きデータの課題に対処し、さまざまな領域で最先端のパフォーマンスを実現する新しいフレームワークです。本アプローチにより、マルチモーダル学習の可能性は一層広がります。

## 参照URL

[^1]:[Multimodal Learning](https://atmarkit.itmedia.co.jp/ait/articles/2207/04/news016.html)
[^2]:[Structured Data](https://e-words.jp/w/%E6%A7%8B%E9%80%A0%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF.html#:~:text=%E6%A7%8B%E9%80%A0%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF%EF%BC%88structured%20data,%E3%81%9F%E3%83%87%E3%83%BC%E3%82%BF%E9%9B%86%E5%90%88%E3%81%AE%E3%81%93%E3%81%A8%E3%80%82)
[^3]:[Unstructured Data](https://ja.wikipedia.org/wiki/%E9%9D%9E%E6%A7%8B%E9%80%A0%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF#:~:text=%E9%9D%9E%E6%A7%8B%E9%80%A0%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF%20%E3%81%95%E3%82%8C%E3%82%8B%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E6%8C%87%E3%81%99%E3%80%82)
[^4]:[Alignment](https://eow.alc.co.jp/search?q=alignment)
[^5]:[Fusion](https://www.autodesk.co.jp/products/fusion-360/overview)
[^6]:[Deep Neural Networks](https://aismiley.co.jp/ai_news/dnn-net-work-cnn/#:~:text=%E3%81%BE%E3%81%A8%E3%82%81-,DNN%EF%BC%88%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%89%E3%81%A8%E3%81%AF,%E3%82%92%E5%8F%AF%E8%83%BD%E3%81%AB%E3%81%97%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82)
[^7]:[Overfitting](https://www.nri.com/jp/knowledge/glossary/lst/ka/overfitting#:~:text=%E9%81%8E%E5%AD%A6%E7%BF%92%E3%81%A8%E3%81%AF,%E3%81%95%E3%82%8C%E3%81%A6%E3%81%97%E3%81%BE%E3%81%86%E3%81%93%E3%81%A8%E3%81%A7%E3%81%99%E3%80%82)
[^8]:[Generalization](https://eow.alc.co.jp/search?q=generalization)
[^9]:[Non-stationary Behavior](https://www.investopedia.com/articles/trading/07/stationary.asp)
[^10]:[Attention-based Architectures](https://machinelearningmastery.com/a-tour-of-attention-based-architectures/)
[^11]:[Cross-attention](https://www.hello-statisticians.com/ml/deeplearning/cross-attention.html)
[^12]:[Transformer Layers](https://qiita.com/halhorn/items/c91497522be27bde17ce)
[^13]:[Masked Language Modeling](https://www.kobelcosys.co.jp/column/itwords/20211101/)
[^14]:[Self-supervised Learning](https://atmarkit.itmedia.co.jp/ait/articles/2302/15/news018.html)
[^15]:[Siamese Networks](https://cpp-learning.com/siamese-network/)
[^16]:[Cosine Similarity](https://atmarkit.itmedia.co.jp/ait/articles/2112/08/news020.html)
[^17]:[Fine-tuning](https://atmarkit.itmedia.co.jp/ait/articles/2301/26/news019.html#:~:text=%E6%9C%8811%E6%97%A5-,%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%EF%BC%88Fine%2Dtuning%EF%BC%9A%E5%BE%AE%E8%AA%BF%E6%95%B4%EF%BC%89%E3%81%A8%E3%81%AF,%E8%AA%BF%E6%95%B4%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%82%92%E6%8C%87%E3%81%99%E3%80%82)
[^18]:[Classification Module](https://www.ibm.com/docs/ja/SS8JHU_2.2.0/com.ibm.content.collector.doc/icm/t_afu_integrate_icm.htm)
[^19]:[Regression Tasks](https://www.sciencedirect.com/science/article/pii/B9780323917766000099)
[^20]:[Retrieval Tasks](https://eow.alc.co.jp/search?q=retrieval+task)
[^21]:[AUROC](https://atmarkit.itmedia.co.jp/ait/articles/2211/24/news019.html)