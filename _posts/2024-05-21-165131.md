---

title: "差分プライバシーでユーザー保護 - 合成データで訓練データを安全に"
categories:
  - テクノロジー
tags:
  - 機械学習
  - プライバシー
  - 合成データ
  - 差分プライバシー
  - LLM

私たちは、差分プライバシーを活用してユーザーのプライバシーを保護する新しいアプローチを紹介します。この方法を使用すると、個人データを漏洩することなく、安全にモデルを訓練することが可能です。

## 差分プライバシーとは？
差分プライバシー[^1]は、データ分析アルゴリズムが個々のユーザーのプライバシーをどれだけ保護するかを定量化する数学的フレームワークです。このフレームワークでは、ユーザーのデータが含まれる場合と含まれない場合の2つのシナリオを考慮します。出力が非常に似ている場合、アルゴリズムはプライベートとされます。

## 合成データの生成
合成データ[^2]は元のデータと同じ特徴を持ちつつも、完全に人工的なデータです。このデータは、元データのプライバシーを強固に保護します。Googleでは、差分プライバシーを利用し、合成データを生成する新しい方法を研究しています。

### 合成データの利用
- **予測モデルの訓練**: 合成データを使用したモデルの訓練には、DP-SGD（Differentially Private Stochastic Gradient Descent）[^3][^4]というプライベートなMLアルゴリズムが使用されます。これにより、ユーザーのプライバシーが保証されます。
- **補助タスク**: 特徴エンジニアリング、モデル選択、ハイパーパラメータ調整[^10]などのタスクにも合成データは活用できます。
- **デバッグと監視**: MLアルゴリズムのデバッグや、長期間にわたる監視とアラートにも使用できます。

## Googleのアプローチ
Googleでは、合成データを生成するために大規模言語モデル（LLM）[^5]を使用し、そのモデルを差分プライバシーを用いてファインチューニング[^11]する方法を開発しています。この方法では、モデルの全ての重みを更新するのではなく、LoRaファインチューニング[^12]やプロンプトファインチューニング[^13]など、パラメータ効率の良い技術[^14]を使用しています。

### 実験結果
Googleの研究では、以下の3つの公開データセットを使用して実験を行いました。
1. IMDB映画レビュー
2. Yelpビジネスレビュー
3. AGニュース記事

LoRaファインチューニングでは、およそ2000万のパラメータが変更され、プロンプトファインチューニングでは約4万1千のパラメータが変更されました。その結果、合成データを使用した場合も高い精度が得られ、元のデータを使用する場合と比較しても性能差はほとんどありませんでした。

### 端末上の安全性分類器への応用
Googleは、端末上における安全なコンテンツ分類のために、このアプローチを応用しています。生成された合成データを使用することで、手作業によるデータのフィルタリングや拡張、混合なども可能となり、分類器の性能向上に寄与しています。

## まとめ
差分プライバシーを活用した合成データ生成は、ユーザーのプライバシーを強固に保護しながら、機械学習モデルの精度も維持する効率的な方法です。Googleの研究は、このアプローチがどのように実際の応用例で有効であるかを示しています。

## 参照URL
[^1]: [Differential privacy](https://www.anonify.layerx.co.jp/post/differential-privacy)
[^2]: [Synthetic data](https://acompany.tech/privacytechlab/synthetic-data-merit-demerit/#:~:text=%E6%9B%B8%E3%81%84%E3%81%A6%E3%81%84%E3%81%8F%E3%80%82-,%E5%90%88%E6%88%90%E3%83%87%E3%83%BC%E3%82%BF%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B,Artificially%20Generated%20Data%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%E3%80%82)
[^3]: [Training data](https://e-words.jp/w/%E6%95%99%E5%B8%AB%E3%83%87%E3%83%BC%E3%82%BF.html#:~:text=%E6%95%99%E5%B8%AB%E3%83%87%E3%83%BC%E3%82%BF%20%E3%80%90training%20data%E3%80%91%20%E3%83%88%E3%83%AC%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0,%E3%83%87%E3%83%BC%E3%82%BF%20%2F%20%E8%A8%93%E7%B7%B4%E3%83%87%E3%83%BC%E3%82%BF%20%2F%20%E6%95%99%E5%B8%AB%E4%BF%A1%E5%8F%B7&text=%E6%95%99%E5%B8%AB%E3%83%87%E3%83%BC%E3%82%BF%EF%BC%88training%20data%EF%BC%89%E3%81%A8,%E3%80%8C%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92%E3%80%8D%E3%81%A8%E3%81%84%E3%81%86%E3%80%82)
[^4]: [Stochastic gradient descent](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)
[^5]: [Large Language Models](https://atmarkit.itmedia.co.jp/ait/articles/2303/13/news013.html)
[^10]: [Hyperparameter tuning](https://aws.amazon.com/jp/what-is/hyperparameter-tuning/#:~:text=%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E3%83%88%E3%83%AC%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0,%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82)
[^11]: [Fine-tuning](https://zero2one.jp/ai-word/finetuning/#:~:text=%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%A8%E3%81%AF%E3%80%81%E5%87%BA%E5%8A%9B,%E3%81%99%E3%82%8B%E5%8A%B9%E6%9E%9C%E3%81%8C%E6%9C%9F%E5%BE%85%E3%81%A7%E3%81%8D%E3%82%8B%E3%80%82)
[^12]: [LoRa fine-tuning](https://qiita.com/DeepMata/items/cb4ff18c1e0548bdb844)
[^13]: [Prompt fine-tuning](https://qiita.com/tmgauss/items/22c4e5e00282a23e569d)
[^14]: [Parameter-efficient techniques](https://arxiv.org/abs/2312.12148)

---