---
title: "生成AIが切り開く自動運転技術の新時代：Wayveの取り組み"
categories:
  - 自動運転
tags:
  - 人工知能
  - 生成AI
  - NVIDIA
---
生成AIが自動運転技術に革命を起こしています。

## AV 2.0とWayveの挑戦

ロンドンに拠点を置くスタートアップWayveは、現実の動的な環境で意思決定ができる自動運転技術を開発しています。これらの技術はNVIDIA DRIVE Orinとその後継機であるNVIDIA DRIVE Thor上で構築されており、NVIDIA Blackwell GPUのアーキテクチャを用いた大規模なAIモデルを活用しています[^1][^2][^3]。

### AV 1.0からAV 2.0への進化

AV 1.0は複数のディープニューラルネットワークを用いた車両の感知能力の向上に焦点を当てていましたが、AV 2.0では車両内の包括的な知能が求められています[^4]。これにより、動的な現実環境での意思決定が可能となります。

WayveはNVIDIA Inceptionプログラムに参加しており、自動運転のためのAI基盤モデルの開発に特化しています。これにより車両に「ロボット脳」を搭載し、周囲の環境から学び相互作用できるようにしています[^5]。

「NVIDIAは私たちがAIを訓練するための酸素のような存在です」と、Wayve共同創業者兼CEOのAlex Kendall氏は述べています。「私たちはNVIDIAのGPU上で訓練を行い、NVIDIAが提供するソフトウェアエコシステムにより迅速なイテレーションが可能となっています。これがペタバイトのデータで訓練された数十億パラメーターモデルの構築を可能にしています」[^6][^7]。

### 生成AIの役割

生成AIはWayveの開発プロセスにおいても重要な役割を果たしています。これにより、従来の経験を基にした新しい運転シナリオのシミュレーションが可能となります[^8]。Wayveは、車両やロボットに高度なAIを統合し、人間の行動から学び反応する「embodied AI」を構築しています[^9]。これにより、安全性が向上します。

## Wayveの新たな展開

Wayveは最近、NVIDIAも参加するシリーズC投資ラウンドを発表し、生産車向けの最初のembodied AI製品の開発と導入をサポートしています。WayveのコアAIモデルが進化するにつれ、これらの製品はL2+の支援運転からL4の自動運転まで、車両の高度な運転自動化への効率的なアップグレードを可能にします[^10]。

### GAIA-1とLINGO-2の発表

Wayveは、現実的な運転ビデオを生成するGAIA-1というモデルを発表しました。このモデルはビデオ、テキスト、アクションの入力を使用して、運転シナリオを創り出します。また、視覚、言語、アクション入力を結びつけて運転行動を説明するLINGO-2というモデルも発表しました[^11]。

「生成AIの素晴らしい点の一つは、異なるデータモードをシームレスに組み合わせることができることです」とKendall氏は述べています。「LLM（大規模言語モデル）から得られる知識や一般的な推論能力、それらを運転に応用することが私たちが求める真の一般化された自律性、最終的にはレベル5（完全自動運転）の能力に到達するための最も有望なアプローチの一つです」[^12]。

## まとめ

Wayveは生成AIを駆使して自動運転技術の新たな時代を切り開いています。NVIDIAの最新技術を活用し、安全で高度な自動運転を実現するための取り組みが続けられています。この進化により、未来の自動車がどのように進化するかが期待されます。

## 参考URL
[^1]:[NVIDIA DRIVE Orin](https://www.nvidia.com/ja-jp/self-driving-cars/in-vehicle-computing/)
[^2]:[NVIDIA DRIVE Thor](https://www.nvidia.com/ja-jp/about-nvidia/press-releases/2022/nvidia-unveils-drive-thor-centralized-car-computer-unifying-cluster-infotainment-automated-driving-and-parking-in-a-single-cost-saving-system/)
[^3]:[NVIDIA Blackwell GPU](https://www.nvidia.com/ja-jp/data-center/technologies/blackwell-architecture/)
[^4]:[deep neural networks](https://aismiley.co.jp/ai_news/dnn-net-work-cnn/#:~:text=%E3%81%BE%E3%81%A8%E3%82%81-,DNN%EF%BC%88%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%89%E3%81%A8%E3%81%AF,%E3%82%92%E5%8F%AF%E8%83%BD%E3%81%AB%E3%81%97%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82}
[^5]:[AI foundation models](https://atmarkit.itmedia.co.jp/ait/articles/2302/27/news014.html)
[^6]:[petabytes](https://www.hitachi-systems-es.co.jp/service/column/backup/article10.html#:~:text=%E3%83%9A%E3%82%BF%E3%83%90%E3%82%A4%E3%83%88%E3%81%A8%E3%81%AF%E3%80%81%E3%83%A1%E3%82%AC%E3%80%81%E3%82%AE%E3%82%AC,%E3%81%AE%E3%82%B1%E3%83%BC%E3%82%B9%E3%81%A7%E5%8D%81%E5%88%86%E3%81%A7%E3%81%99%E3%80%82)
[^7]:[LLM](https://www.nec-solutioninnovators.co.jp/sp/contents/column/20240229_llm.html)
[^8]:[synthetic data generation](https://ja.scoville.jp/digital-products/sdg#:~:text=SDG%E3%81%AF%E3%80%81%E6%A7%98%E3%80%85%E3%81%AA%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88,%E3%81%AA%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7%E3%81%99%E3%80%82&text=ai%20%E7%8B%AC%E8%87%AA%E3%81%AE%E5%AD%A6%E7%BF%92%E3%82%92,%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%97%E3%81%BE%E3%81%99%E3%80%82)
[^9]:[embodied AI](https://speakerdeck.com/nttcom/about-embodied-ai)
[^10]:[AV 2.0](https://store.shopping.yahoo.co.jp/densenyasan/05120001n.html)
[^11]:[L5 capabilities](https://github.com/opdev/l5-operator-demo)
