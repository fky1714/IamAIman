---
title: "USER-LLM: 効率的なLLMコンテクスト化とユーザーエンベディングの活用"
categories:
  - 生成AI
tags:
  - 自然言語処理
  - パーソナライズ
---
USER-LLMとは、ユーザーの多様なインタラクションをユーザーエンベディング[^1]として蒸留し、これをLLM（大規模言語モデル）にコンテクスト化するフレームワークです。この新しいアプローチにより、ユーザーの行動パターンと嗜好を深く理解し、効率的なパーソナライズを実現します。

## はじめに

大規模言語モデル（LLMs）は自然言語処理（NLP）の分野で革命的な進展を遂げており、膨大なテキストデータから学習し適応する能力を持っています。これにより、ユーザーの行動を理解し、パーソナライズされたサービスを提供するための大きな可能性が広がっています。しかし、ユーザーインタラクションデータは複雑であり、多様なデータポイントやノイズの問題を抱えているため、LLMsの効果的な利用は困難です。

## USER-LLMの紹介

USER-LLMは、ユーザーの多様なインタラクションを圧縮し、ユーザーエンベディングとして表現することで、LLMsの能力を強化します。このアプローチにより、以下の点でLLMsが向上します。

1. ノイズや複雑さを乗り越え、関連するパターンを簡潔に把握
2. ユーザーの意図や行動の文脈を深く理解
3. 長いインタラクション履歴を効率的に処理し、計算資源の消耗を緩和

### USER-LLMの設計

USER-LLMは、マルチモーダルなIDベースの特徴を使用してトランスフォーマーベースのエンコーダ[^2]でユーザーエンベディングを生成します。これらの特徴は単一のエンベディングに結合され、エンコーダに入力されます。自己回帰トランスフォーマーモデル[^3]を使用して、ユーザーの活動シーケンスから次のトークンを予測します。

次に、USER-LLMはLLMとの統合を行い、クロスアテンション[^4]を使用します。事前学習エンコーダの出力エンベディングは、LLM内の中間テキスト表現とクロスアテンションされます。また、プレフィクスエンベディングとしてソフトプロンプト[^5]をプレンドする方法にも対応しています。さらに、プロジェクションレイヤーにPerceiverユニットを統合し、ユーザーエンベディングを圧縮して効率を最適化します。

## パフォーマンスと効率性

USER-LLMのパフォーマンスと効率性を評価するために、MovieLens20M、Google Local Review、Amazon Reviewの3つの公開データセットを使用し、次のアイテム予測、好きなカテゴリ予測、次のレビュー生成のタスクを実施しました。これらのタスクにおいて、USER-LLMは、従来のテキストプロンプトベースのコンテクスト化手法を上回る性能を示しました。

特に、長いシーケンスの処理では、USER-LLMはテキストプロンプトアプローチよりも優れた結果を示し、計算コストやメモリ要求の面でも効率的でした。[下図]ではテキストプロンプトアプローチと比べた場合の計算効率が強調されています。

### 比較結果

USER-LLMは、次のアイテム予測タスクにおいて、テキストプロンプトベースのアプローチを上回り、シーケンス長が増加するほど性能が向上しました。

- 次のアイテム予測タスクでは、USER-LLMがBert4Recやデュアルエンコーダのようなベースラインモデルを上回る性能を示しました。
- お気に入りジャンルやカテゴリの予測、およびレビュー生成タスクにおいても、USER-LLMは競合するベースラインを上回る競争力のある結果を示しました。

## まとめ

本研究では、USER-LLMという革新的なフレームワークを提案しました。このフレームワークは、ユーザーエンベディングを活用してLLMsをコンテクスト化することで、効果的なユーザーモデリングとパーソナライズを実現します。複数のデータセットにわたる広範な評価により、USER-LLMは深いユーザー理解と長いシーケンスモデリングが求められるタスクにおいて、優れた性能と計算効率を示しました。

今後は、ユーザーエンベディングを最適化し、LLM空間とのアライメントを強化し、多様なタスクでの訓練を行うことで、USER-LLMの堅牢性と多様性をさらに向上させる可能性があります。本研究は、ユーザーモデリングとLLMパーソナライズの強力なフレームワークとしての地位を確立します。

## 参考URL

[^1]: [User Embeddings](https://arxiv.org/abs/2402.13598)
[^2]: [Transformer-Based Encoder](https://www.jstage.jst.go.jp/article/jsaislud/90/0/90_24/_pdf)
[^3]: [Autoregressive Transformer](https://techblog.yahoo.co.jp/entry/2020122230052967/)
[^4]: [Cross-Attention](https://www.hello-statisticians.com/ml/deeplearning/cross-attention.html)
[^5]: [Soft-Prompting](https://webbigdata.jp/post-12756/)
[^6]: [Multimodal Data](https://atmarkit.itmedia.co.jp/ait/articles/2207/04/news016.html)
[^7]: [Feature Space Reconstruction](https://pubmed.ncbi.nlm.nih.gov/35469205/)
[^8]: [Inference Efficiency](https://towardsdatascience.com/efficient-inference-in-deep-learning-where-is-the-problem-4ad59434fe36)
[^9]: [FLOPs (Floating Point Operations)](https://ja.wikipedia.org/wiki/FLOPS)

